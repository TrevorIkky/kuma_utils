{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial: Train PyTorch Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = datasets.load_breast_cancer(return_X_y=True)\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(features, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "((569, 30), (569,))"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "features.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "torch_xla not found.\nnvidia apex not found.\n"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "from kuma_utils.torch import TorchTrainer, EarlyStopping, TorchLogger\n",
    "from kuma_utils.torch.model_zoo import TabularNet\n",
    "from kuma_utils.metrics import AUC, Accuracy\n",
    "from kuma_utils.utils import sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = D.TensorDataset(\n",
    "    torch.as_tensor(x_train).float(),\n",
    "    torch.as_tensor(y_train.reshape(-1, 1)).float()\n",
    ")\n",
    "valid_ds = D.TensorDataset(\n",
    "    torch.as_tensor(x_valid).float(),\n",
    "    torch.as_tensor(y_valid.reshape(-1, 1)).float()\n",
    ")\n",
    "train_loader = D.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "valid_loader = D.DataLoader(valid_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "TorchLogger created at 20/12/01:09:42:09\n"
    }
   ],
   "source": [
    "model = TabularNet(in_features=x_train.shape[1], out_features=1)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=5e-3)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', factor=0.5, patience=2, verbose=True, min_lr=1e-5)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "LogitsAcc = lambda approx, target: Accuracy().torch(sigmoid(approx), target)\n",
    "\n",
    "fit_params = {\n",
    "    'loader': train_loader,\n",
    "    'loader_valid': valid_loader,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'num_epochs': 30,\n",
    "    'callbacks': [EarlyStopping(5, target='valid_metric', maximize=True, skip_epoch=10)],\n",
    "    'eval_metric': AUC().torch,\n",
    "    'monitor_metrics': [LogitsAcc], \n",
    "    'logger': TorchLogger(\n",
    "        'logger.log', \n",
    "        log_items=[\n",
    "            'epoch', 'train_loss', 'valid_loss', \n",
    "            'valid_metric', 'valid_monitor', 'patience'], stdout=True, file=False),\n",
    "    # 'calibrate_model': True, \n",
    "    'export_dir': 'results/baseline/',\n",
    "    # 'resume': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "09:42:39 Model is on cpu\n09:42:39 [Epoch   1/ 30] train_loss=4.602656 | valid_loss=0.706972 | valid_metric=0.219805 | valid_monitor=[0.606033] \n09:42:39 [Epoch   2/ 30] train_loss=2.522258 | valid_loss=0.696433 | valid_metric=0.236039 | valid_monitor=[0.574023] \n09:42:39 [Epoch   3/ 30] train_loss=1.759894 | valid_loss=0.630515 | valid_metric=0.953571 | valid_monitor=[0.407972] \n09:42:39 [Epoch   4/ 30] train_loss=1.157770 | valid_loss=0.692414 | valid_metric=0.334091 | valid_monitor=[0.612034] \n09:42:39 [Epoch   5/ 30] train_loss=0.916939 | valid_loss=0.668145 | valid_metric=0.549675 | valid_monitor=[0.580025] \n09:42:40 [Epoch   6/ 30] train_loss=0.771432 | valid_loss=0.674632 | valid_metric=0.833442 | valid_monitor=[0.481994] \nEpoch     7: reducing learning rate of group 0 to 5.0000e-04.\n09:42:40 [Epoch   7/ 30] train_loss=0.681121 | valid_loss=0.640791 | valid_metric=0.853247 | valid_monitor=[0.528009] \n09:42:40 [Epoch   8/ 30] train_loss=0.689128 | valid_loss=0.639499 | valid_metric=0.770779 | valid_monitor=[0.570022] \n09:42:40 [Epoch   9/ 30] train_loss=0.639909 | valid_loss=0.625989 | valid_metric=0.779545 | valid_monitor=[0.570022] \n09:42:40 [Epoch  10/ 30] train_loss=0.610972 | valid_loss=0.592914 | valid_metric=0.877922 | valid_monitor=[0.526008] \n09:42:40 [Epoch  11/ 30] train_loss=0.602967 | valid_loss=0.567577 | valid_metric=0.795779 | valid_monitor=[0.554017] \n09:42:40 [Epoch  12/ 30] train_loss=0.618552 | valid_loss=0.564970 | valid_metric=0.840584 | valid_monitor=[0.542013] \n09:42:40 [Epoch  13/ 30] train_loss=0.573667 | valid_loss=0.566893 | valid_metric=0.893831 | valid_monitor=[0.534010] \n09:42:40 [Epoch  14/ 30] train_loss=0.585800 | valid_loss=0.565343 | valid_metric=0.781169 | valid_monitor=[0.572022] (*1)\n09:42:40 [Epoch  15/ 30] train_loss=0.562763 | valid_loss=0.531283 | valid_metric=0.857143 | valid_monitor=[0.542013] (*2)\n09:42:40 [Epoch  16/ 30] train_loss=0.544253 | valid_loss=0.511088 | valid_metric=0.893831 | valid_monitor=[0.534010] (*3)\n09:42:40 [Epoch  17/ 30] train_loss=0.534571 | valid_loss=0.491962 | valid_metric=0.847727 | valid_monitor=[0.548015] (*4)\n09:42:40 [Epoch  18/ 30] train_loss=0.505620 | valid_loss=0.482374 | valid_metric=0.925000 | valid_monitor=[0.516005] \n09:42:40 [Epoch  19/ 30] train_loss=0.512926 | valid_loss=0.481796 | valid_metric=0.857143 | valid_monitor=[0.552016] (*1)\n09:42:40 [Epoch  20/ 30] train_loss=0.500312 | valid_loss=0.432875 | valid_metric=0.901623 | valid_monitor=[0.536011] (*2)\n09:42:40 [Epoch  21/ 30] train_loss=0.500773 | valid_loss=0.453543 | valid_metric=0.907143 | valid_monitor=[0.536011] (*3)\n09:42:40 [Epoch  22/ 30] train_loss=0.528568 | valid_loss=0.448959 | valid_metric=0.898377 | valid_monitor=[0.536011] (*4)\n09:42:40 [Epoch  23/ 30] train_loss=0.521389 | valid_loss=0.442618 | valid_metric=0.815909 | valid_monitor=[0.566020] (*5)\n09:42:40 Training stopped by overfit detector.\n09:42:40 Best epoch is [18], best score is [0.925].\n09:42:40 Prediction done. exported to None\n09:42:40 Skipping prediction...\n"
    }
   ],
   "source": [
    "trn = TorchTrainer(model)\n",
    "trn.train(**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "array([[ 0.5846777 ],\n       [-0.8197745 ],\n       [-0.03987291],\n       [-0.25906694],\n       [-1.9227417 ]], dtype=float32)"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "trn.outoffold[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1606810193829",
   "display_name": "Python 3.7.9 64-bit ('kumaconda': pyenv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}